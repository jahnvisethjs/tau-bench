# Copyright Sierra
# CORRECT s1 Budget Forcing Implementation - Token Level Control

import json
import re
from typing import Optional, List, Dict, Any, Tuple

try:
    from vllm import LLM, SamplingParams
    from vllm.outputs import RequestOutput
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False

from litellm import completion
import tiktoken

from tau_bench.agents.base import Agent
from tau_bench.envs.base import Env
from tau_bench.types import (
    Action,
    SolveResult,
    RESPOND_ACTION_NAME,
    RESPOND_ACTION_FIELD_NAME,
)


class ChatReActAgent(Agent):
    def __init__(
        self,
        tools_info: List[Dict[str, Any]],
        wiki: str,
        model: str,
        provider: str,
        use_reasoning: bool = True,
        temperature: float = 0.0,
        token_budget: Optional[int] = None,
        enable_wait_tokens: bool = False,
        max_num_steps: int = 30,
        num_wait_tokens: int = 2,
        use_vllm: bool = False,
    ) -> None:
        instruction = REACT_INSTRUCTION if use_reasoning else ACT_INSTRUCTION
        self.model = model
        self.provider = provider
        self.temperature = temperature
        self.use_reasoning = use_reasoning
        self.tools_info = tools_info
        self.token_budget = token_budget
        self.use_vllm = use_vllm and VLLM_AVAILABLE
        self.enable_budget_forcing = enable_wait_tokens
        
        # Add budget constraint prompt
        if self.token_budget is not None:
            minimum_budget = int(self.token_budget * 0.5)
            budget_constraint = f"""

# Reasoning Budget Guidance

You have up to {self.token_budget} tokens for reasoning. For complex tasks, aim to use at least {minimum_budget} tokens to ensure thorough analysis.

Best practices:
- Think step-by-step through the problem
- Verify your understanding before taking actions
- Double-check tool outputs and policy requirements
- Consider edge cases and potential errors
"""
        else:
            budget_constraint = ""
        
        self.prompt = (
            wiki + "\n#Available tools\n" + json.dumps(tools_info) + budget_constraint + instruction
        )
        self.max_num_steps = max_num_steps
        self.num_wait_tokens = num_wait_tokens
        self.total_tokens = 0
        
        # Initialize vLLM if requested
        if self.use_vllm:
            if not VLLM_AVAILABLE:
                raise ImportError("vLLM not installed: pip install vllm")
            print(f"üöÄ Initializing vLLM with model: {model}")
            self.vllm_model = LLM(
                model=model,
                trust_remote_code=True,
                max_model_len=8192,
                gpu_memory_utilization=0.9,
            )
            self.vllm_tokenizer = self.vllm_model.get_tokenizer()
        else:
            self.vllm_model = None
            self.vllm_tokenizer = None
        
        # Initialize tokenizer for token counting
        try:
            self.tokenizer = tiktoken.encoding_for_model(self.model)
        except:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def _format_messages_for_vllm(self, messages: List[Dict[str, Any]]) -> str:
        """Convert message list to prompt string for vLLM."""
        prompt_parts = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            if role == "system":
                prompt_parts.append(f"{content}\n\n")
            elif role == "user":
                prompt_parts.append(f"User: {content}\n\n")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}\n\n")
        
        prompt_parts.append("Assistant: ")
        return "".join(prompt_parts)

    def _is_trying_to_respond(self, text: str) -> bool:
        """Check if the generated text contains a respond action."""
        if '"name": "respond"' in text or '"name":"respond"' in text:
            return True
        if '"name": respond' in text:
            return True
        return False

    def _extract_action_from_response(self, response_text: str) -> Tuple[str, Dict]:
        """Extract action from model response text."""
        if "Action:" not in response_text:
            return None, None
        
        action_str = response_text.split("Action:")[-1].strip()
        
        try:
            action_parsed = json.loads(action_str)
            return action_str, action_parsed
        except json.JSONDecodeError:
            action_str_fixed = re.sub(
                r'"name":\s*([a-zA-Z_][a-zA-Z0-9_]*)',
                r'"name": "\1"',
                action_str
            )
            try:
                action_parsed = json.loads(action_str_fixed)
                return action_str_fixed, action_parsed
            except:
                return action_str, None

    def generate_with_budget_forcing_vllm(
        self, 
        messages: List[Dict[str, Any]], 
        minimum_budget: Optional[int] = None
    ) -> Tuple[Dict[str, Any], Action, float, int]:
        """
        CORRECT s1 BUDGET FORCING IMPLEMENTATION
        
        This implements budget forcing by:
        1. Generating with stop tokens that detect "Action:" pattern
        2. When model tries to respond early, checking if minimum budget is met
        3. If not met, suppressing the stop and appending "Wait" token
        4. Continuing generation from the extended prompt
        
        Key insight: We detect when the model WANTS to stop (by generating "Action:"),
        then suppress that by continuing generation with "Wait" appended.
        """
        prompt = self._format_messages_for_vllm(messages)
        
        # Calculate available token budget
        if self.token_budget:
            max_new_tokens = self.token_budget - self.total_tokens
            max_new_tokens = max(100, min(max_new_tokens, 2048))
        else:
            max_new_tokens = 2048
        
        # CRITICAL: Define stop strings that indicate model wants to take action
        # In ReAct format, "Action:" signals the model wants to act
        stop_strings = ["Action:", "\n\n\n"]
        
        generated_text = ""
        total_tokens_generated = 0
        forced_continuations = 0
        max_forced_continuations = self.num_wait_tokens
        
        # Iterative generation with budget forcing
        while total_tokens_generated < max_new_tokens:
            chunk_size = min(2048, max_new_tokens - total_tokens_generated)
            
            if chunk_size < 50:
                break
            
            # Generate until we hit a stop string or max tokens
            sampling_params = SamplingParams(
                temperature=self.temperature,
                max_tokens=chunk_size,
                stop=stop_strings,
                skip_special_tokens=False,
            )
            
            current_prompt = prompt + generated_text
            outputs = self.vllm_model.generate([current_prompt], sampling_params)
            chunk_text = outputs[0].outputs[0].text
            finish_reason = outputs[0].outputs[0].finish_reason
            
            chunk_tokens = len(self.tokenizer.encode(chunk_text))
            generated_text += chunk_text
            total_tokens_generated += chunk_tokens
            
            # Check if we stopped because of "Action:" (model wants to respond)
            if finish_reason == "stop" and any(stop_str in chunk_text for stop_str in stop_strings[:-1]):
                # Model is trying to take action - check budget forcing
                current_total = self.total_tokens + total_tokens_generated
                
                if (self.enable_budget_forcing and 
                    minimum_budget and 
                    current_total < minimum_budget and
                    forced_continuations < max_forced_continuations):
                    
                    # BUDGET FORCING: Suppress the stop by continuing with "Wait"
                    print(f"‚ö†Ô∏è  Budget forcing at {current_total} tokens (minimum: {minimum_budget})")
                    print(f"   Forcing continuation {forced_continuations + 1}/{max_forced_continuations}")
                    
                    # CRITICAL: Append "Wait" as per s1 paper
                    # This simulates suppressing the end-of-thinking token
                    wait_token = " Wait"
                    generated_text += wait_token
                    total_tokens_generated += len(self.tokenizer.encode(wait_token))
                    forced_continuations += 1
                    
                    # Continue generating - model will reconsider
                    continue
                else:
                    # Budget met or max continuations reached
                    if current_total >= minimum_budget:
                        print(f"‚úÖ Minimum budget reached ({current_total} >= {minimum_budget})")
                    else:
                        print(f"‚ö†Ô∏è  Max continuations reached at {current_total} tokens")
                    break
            
            # If finished naturally (length limit, other stop), break
            if finish_reason == "length":
                break
        
        # Extract final action
        action_str, action_parsed = self._extract_action_from_response(generated_text)
        
        if not action_parsed:
            action_parsed = {
                "name": RESPOND_ACTION_NAME,
                "arguments": {RESPOND_ACTION_FIELD_NAME: generated_text},
            }
        
        action = Action(name=action_parsed["name"], kwargs=action_parsed["arguments"])
        
        message = {
            "role": "assistant",
            "content": generated_text,
        }
        
        return message, action, 0.0, total_tokens_generated

    def generate_next_step_litellm(
        self, messages: List[Dict[str, Any]]
    ) -> Tuple[Dict[str, Any], Action, float, int]:
        """Fallback to LiteLLM (no budget forcing)."""
        res = completion(
            model=self.model,
            custom_llm_provider=self.provider,
            messages=messages,
            temperature=self.temperature,
        )
        message = res.choices[0].message
        action_str = message.content.split("Action:")[-1].strip()
        
        try:
            action_parsed = json.loads(action_str)
        except json.JSONDecodeError:
            action_str_fixed = re.sub(
                r'"name":\s*([a-zA-Z_][a-zA-Z0-9_]*)',
                r'"name": "\1"',
                action_str
            )
            try:
                action_parsed = json.loads(action_str_fixed)
            except:
                action_parsed = {
                    "name": RESPOND_ACTION_NAME,
                    "arguments": {RESPOND_ACTION_FIELD_NAME: action_str},
                }
        
        action = Action(name=action_parsed["name"], kwargs=action_parsed["arguments"])
        token_count = len(self.tokenizer.encode(message.content))
        
        return message.model_dump(), action, res._hidden_params.get("response_cost", 0.0), token_count

    def solve(self, env: Env, task_index: Optional[int] = None) -> SolveResult:
        response = env.reset(task_index=task_index)
        
        self.total_tokens = 0
        reward = 0.0
        messages: List[Dict[str, Any]] = [
            {"role": "system", "content": self.prompt},
            {"role": "user", "content": response.observation},
        ]
        total_cost = 0.0
        info = {}
        
        # Calculate minimum budget
        minimum_budget = None
        if self.token_budget and self.enable_budget_forcing:
            minimum_budget = int(self.token_budget * 0.5)
            print(f"üìä Budget forcing enabled: {minimum_budget}/{self.token_budget} tokens (min/max)")
        
        for step_num in range(self.max_num_steps):
            # Generate next step
            if self.use_vllm:
                message, action, cost, token_count = self.generate_with_budget_forcing_vllm(
                    messages, minimum_budget
                )
            else:
                message, action, cost, token_count = self.generate_next_step_litellm(messages)
            
            self.total_tokens += token_count
            
            # Check maximum budget
            budget_exceeded = (self.token_budget is not None and 
                             self.total_tokens >= self.token_budget)
            
            if budget_exceeded:
                info["budget_exceeded"] = True
                info["total_tokens_used"] = self.total_tokens
                print(f"‚ùå Maximum budget exceeded: {self.total_tokens}/{self.token_budget}")
                
                if action.name != RESPOND_ACTION_NAME:
                    forced_action = Action(
                        name=RESPOND_ACTION_NAME,
                        arguments={RESPOND_ACTION_FIELD_NAME: "I've reached my processing limit."}
                    )
                    response = env.step(forced_action)
                    reward = response.reward
                    info = {**info, **response.info.model_dump()}
                    break
                else:
                    response = env.step(action)
                    obs = response.observation
                    reward = response.reward
                    info = {**info, **response.info.model_dump()}
                    messages.extend([message, {"role": "user", "content": obs}])
                    break
            
            # Execute action
            response = env.step(action)
            obs = response.observation
            reward = response.reward
            info = {**info, **response.info.model_dump()}
            
            if action.name != RESPOND_ACTION_NAME:
                obs = "API output: " + obs
            
            messages.extend([
                message,
                {"role": "user", "content": obs},
            ])
            
            total_cost += cost
            
            if response.done:
                break
        
        info["total_tokens_used"] = self.total_tokens
        
        return SolveResult(
            messages=messages,
            reward=reward,
            info=info,
        )


REACT_INSTRUCTION = """
# Instruction
You need to act as an agent that use the above tools to help the user according to the above policy.

At each step, your generation should have exactly the following format:
Thought:
<A single line of reasoning to process the context and inform the decision making. Do not include extra lines.>
Action:
{"name": <The name of the action>, "arguments": <The arguments to the action in json format>}

The Action will be parsed, so it must be valid JSON.

CRITICAL JSON FORMATTING RULES:
- ALL string values MUST be in double quotes
- Action names MUST be quoted strings
- CORRECT: {"name": "respond", "arguments": {"content": "Hello"}}
- WRONG: {"name": respond, "arguments": {"content": "Hello"}}

IMPORTANT GUIDELINES:
1. If you need information from the user that they haven't provided, ask them using the respond action
2. Before calling transfer_to_human_agents, try at least 3-4 different approaches
3. If a tool returns an error, analyze WHY it failed and try a different approach
4. Do not make up or assume user information - always ask if uncertain
5. When searching for users, if one method fails, try different search methods before giving up

You should not use made-up or placeholder arguments.

Try to be helpful and always follow the policy.
"""


ACT_INSTRUCTION = """
# Instruction
You need to act as an agent that use the above tools to help the user according to the above policy.

At each step, your generation should have exactly the following format:

Action:
{"name": <The name of the action>, "arguments": <The arguments to the action in json format>}

The Action will be parsed, so it must be valid JSON.

Try to be helpful and always follow the policy.
"""